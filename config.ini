[server]
server_ai = llama
; ollama / LM studio / hf / llama
; These params only for LM Studio
base_url = http://127.0.0.1:1234/
api_key = for_local_not_needed

[ai_config]
persona = lilith_persona.txt
memory = memory.json
ai_model = gemma3
; ai_model = deepseek-r1  ;;; Ollama model
temperature = 0.85
max_tokens = 120
; Settings for local models
model_path = models/
local_model = gemma-3-4b-q4_k_m.gguf
max_history_messages = 40

[lilith_display]
display_path = modules/viewer.py
revert_delay = 5
blink_min_interval = 4
blink_max_interval = 8
blink_duration = 0.1
assets_path = assets/
default_state = idle
; room / glass
place = room

[viewer_socket]
host = localhost
port = 8888

[translator]
enable = false
source_lang = en
target_lang = ru
